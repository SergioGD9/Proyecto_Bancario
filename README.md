
# ConfiguraciÃ³n de ClÃºster Big Data con Hadoop y Spark

Este proyecto contiene un script para configurar un clÃºster Big Data utilizando Hadoop y Spark en modo pseudo-distribuido.

## ğŸ› ï¸ Requisitos previos
- Ubuntu 20.04 o similar
- Usuario con permisos de sudo
- ConexiÃ³n a Internet

## ğŸš€ Pasos para la configuraciÃ³n

1. **Actualizar el sistema**  
   El script actualizarÃ¡ los paquetes instalados en tu mÃ¡quina.  

2. **Instalar Java**  
   Hadoop requiere Java para funcionar. El script instala OpenJDK 8 y configura las variables de entorno necesarias.

3. **Instalar Hadoop**  
   Descarga, instala y configura Hadoop para modo pseudo-distribuido, incluyendo la configuraciÃ³n de HDFS, YARN y MapReduce.

4. **Instalar Spark**  
   Descarga e instala Apache Spark y configura las variables de entorno para facilitar su uso.

5. **Configurar SSH sin contraseÃ±a**  
   Se genera una clave SSH y se configura la autenticaciÃ³n sin contraseÃ±a para que Hadoop y Spark puedan comunicarse entre nodos.

6. **Formatear el sistema de archivos HDFS**  
   Se inicializa HDFS para almacenar datos en el clÃºster.

7. **Iniciar servicios**  
   Se inician los servicios de Hadoop y Spark.

## ğŸ’» Uso del clÃºster
Una vez configurado, puedes usar el clÃºster para procesamiento de datos masivos. Ejemplo: ejecutar scripts de PySpark.

## ğŸ“ Notas
- Este script configura un entorno de desarrollo en modo pseudo-distribuido (un solo nodo). Para un clÃºster real, aÃ±ade mÃ¡s nodos y ajusta las configuraciones.
- AsegÃºrate de revisar los archivos de configuraciÃ³n en `HADOOP_HOME/etc/hadoop` para personalizar el entorno.

## ğŸ“‚ Archivos en este repositorio
- `setup_cluster.sh`: Script de configuraciÃ³n del clÃºster.
- `README.md`: Este archivo.

## ğŸ¤ Contribuciones
Â¡Las contribuciones son bienvenidas! Si tienes sugerencias o mejoras, no dudes en abrir un pull request.

---

**Autor:** [Tu Nombre]  
**Licencia:** MIT
